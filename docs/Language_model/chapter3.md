# ReNN系列论文泛读
## 3.1 2011-RAE
- 用于预测情感分布的半监督递归自编码器</br>
(Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions)
### 3.1.1 任务目的
- RAE目的解决以下三个问题：
    - 模型利用分层结构和组合语义学来替代词袋表示，从而完成情感分类任务；
    - 在无标签数据或有监督的情感数据集上，不需要特定领域的情感词典、语法程序等条件，模型也能够训练；
    - 对于多种复杂的、相互关联的情感，我们预测了一个多维度的分布，而不是仅仅限制在积极/消极。
### 3.1.2 半监督递归自编码器
#### 神经网络词表示
- 我们将单词表示为一个带参数的连续向量，做如下两个设置：
    - 从一个0均值高斯分布中采样来进行初始化：
    $$x{\sim}\mathcal{N}(0,\sigma^2),x\in\mathbb{R}^n$$
    词向量组成一个词嵌入矩阵( $|V|$ 是词汇表的大小)：
    $$L\in\mathbb{R}^{n{\times}|V|}$$
    - 通过一个无监督的神经网络语言模型来进行预训练来学习一个词嵌入，从共现统计学中捕获句法和语义信息。
- 词向量 $x_i$ 可由词嵌入矩阵 $L$ 和一个独热向量 $b$ 乘积得到：
$$x_i=Lb_k\in\mathbb{R}^n$$
- 我们用有序列表 $(x_1,\cdots,x_m)$ 来表示一个句子(或n-gram)。

#### 传统递归自编码器

![](./img/3.1.1RAE.png ':size=50%')
- 以上图举例，给定一个先验的树结构，传统的递归自编码器通过在树结构上构建维度不变的函数变换，来递归的计算子节点 $c_1$ 和 $c_2$ 的父节点 $p$：
$$p=f(W^{(1)}[c_1;c_2]+b^{(1)})$$
其中 $W^{(1)}$ 是权重矩阵，$b^{(1)}$ 是偏置。然后引入一个逐元素的激活函数，来引入非线性。
- 通过构建一个重构层，来评估模型提取词嵌入的能力：
$$[c_1^{\prime};c_2^{\prime}]=W^{(2)}p+b^{(2)}$$
- 训练目标是最小化重构误差，例如欧式距离：
$$E_{rec}([c_1;c_2])=\frac{1}{2}\left\|[c_1;c_2]-[c_1^{\prime};c_2^{\prime}]\right\|^2$$

#### 为了结构预测的无监督递归自编码器
- 假定对于输入向量 $x$，没有先验的树结构。我们定义由句子 $x$ 生成的所有可能的树结构的集合为 $A(x)$。令 $T(y)$ 是由非叶子节点 $s$ 组成的三元组，使用欧式距离计算重构误差，我们有：
$${\rm{RAE}}_\theta(x)=\mathop{\rm{argmin}}\limits_{y{\in}A(x)}\sum\limits_{s{\in}T(y)}E_{rec}([c_1;c_2]_s)$$
###### 贪心无监督RAE
- 由 $m$ 个单词组成的句子，从句首开始，定义临近的两个词 $(c_1;c_2)=(x_1;x_2)$ 组成的词组为潜在父节点，我们存储可能的父节点 $p$，以及重构误差。
- 计算好后移动一格继续计算 $(c_1;c_2)=(x_2;x_3)$，直到句子结尾 $(c_1;c_2)=(x_{m-1};x_m)$。用最小的重构误差以及对应的潜在父节点来贪心的定义新句子。
- 举例来说，假定句子 $(x_1,x_2,x_3,x_4)$ 的最小重构误差为单词对 $(x_3;x_4)$，那么新句子为 $(x_1,x_2,p_{(3,4)})$。继续递归可能得到 $(x_1,p_{(2,(3,4))})$ 或 $(p_{(1,2)},p_{(3,4)})$。

###### 权重重构
- 词组的单词数不同，我们更希望对词组分配更多的关注，为此我们给重构误差分配权重：
$$E_{rec}([c_1;c_2];\theta)=\frac{n_1}{n_1+n_2}\|c_1-c_1^{\prime}\|^2+\frac{n_2}{n_1+n_2}\|c_2-c_2^{\prime}\|^2$$
其中 $[n_1;n_2]$ 是 $[c_1;c_2]$ 的单词数。

###### 长度归一化
- RAE对词组和词在树中的高度来最小化重构误差，由于其计算隐藏层参数后重构误差，RAE可以通过仅仅降低参数大小来降低误差，这是我们不希望发生的情况。
- 因此调整隐藏层来使得父节点 $p$ 的向量表示模长为1：
$$p=\frac{p}{\|p\|}$$

#### 半监督递归自编码器
- RAE的主要优点之一是，由RAE构建的树的每个节点都有一个分布式向量表示，它也可以被看作描述该短语的特征。我们可以利用这种表示方式，在每个父节点的顶部添加一个简单的softmax层来预测分类的分布：
$$d(p;\theta)={\rm{softmax}}(W^{label}p)$$
- 假定有 $K$ 个标签，$d\in\mathbb{R}^K$ 是一个 $K$ 维多项分布，且有 $\sum_{k=1}^Kd_k=1$。
- 下图展示了这样的半监督RAE单元：

![](./img/3.1.2RAE.png ':size=60%')
- 我们的分类器的目标是一个多项标签分布 $t$，第 $k$ 项用 $t_k$ 来表示，softmax输出一个条件分布 $d_k=p(k|[c_1;c_2])$，因此交叉熵损失为($-p{\rm{log}}q$)：
$$E_{cE}(p,t;\theta)=-\sum\limits_{k=1}^Kt_k{\rm{log}}d_k(p;\theta)$$
- 最终我们的RAE预测句子和标签的组合，通过优化下式来训练：
$$J=\frac{1}{N}\sum\limits_{(x,t)}E(x,t;\theta)+\frac{\lambda}{2}\|\theta\|^2$$
其中第一部分由贪心算法来求得：
$$E(x,t;\theta)=\sum\limits_{s{\in}T(RAE_\theta(x))}E([c_1;c_2]_s,p_s,t,\theta)$$
每个非叶子节点处的误差为重构误差和交叉熵误差的加权和：
$$E([c_1;c_2]_s,p_s,t,\theta)={\alpha}E_{rec}([c_1;c_2]_s;\theta)+(1-\alpha)E_{cE}(p_s,t;\theta)$$
其中 $\alpha$ 是超参数。
- 为了使用该模型预测句子的情感分布，我们使用学习到的树节点的向量表示训练一个简单的逻辑回归分类器。
### 3.1.3 训练
- 令 $\theta=(W^{(1)},b^{(1)},W^{(2)},b^{(2)},W^{label},L),$ 是模型参数，那么梯度为：
$$\frac{{\partial}J}{{\partial}\theta}=\frac{1}{N}\sum\limits_{(x,t)}\frac{{\partial}E(x,t;\theta)}{{\partial}\theta}+\lambda\theta$$
- 由于我们使用贪心算法构造树结构，在计算梯度的时候就不一定是连续的，在梯度下降的方向上不一定会降低目标函数。
- 然而，我们发现L-BFGS在完整的训练数据(批处理模式)上运行以最小化目标在实际中效果很好，并且收敛是平滑的，算法通常能快速找到一个好的解。


## 3.2 2012-MV-RNN
- 通过递归的矩阵-向量空间的语义组合性</br>
(Semantic Compositionality through Recursive Matrix-Vector Spaces)
### 3.2.1 MV-RNN 递归矩阵-向量模型
- 从单个词向量表示构建词组表示的主要方法是将词组表示成单个词向量的线性组合，如求和或加权平均。当词组恰好为其各个部分意义加一起时有效，但如果是一个词修饰另一个词的情况就会失效。
- 一种可能的方法是通过MLP来构造词组的表示：
$$p=g(W[a,b]^{\top})$$
其中 $p$ 是父节点，$g$ 是激活函数，$W$ 是权重矩阵。然后通过构造二叉树来应用上述方法递归计算。但通过一个相同的 $W$ 很难同时让所有词组的向量表示成立。
- 另一种通过矩阵建模来捕获语义向量空间中的信息。但这些方法仅限于捕获单词对的线性函数，而我们希望通过非线性函数来计算多词短语或完整句子的组合意义表示。
- MV-RNN吸取了这些方法的长处：
    - 对每个单词指定一个向量和一个矩阵
    - 学习一个特定输入的、非线性的复合函数，用于计算任意句法类型的多词序列的向量和矩阵表示。

#### 矩阵-向量神经网络词表示
- 我们通过一个50维词向量预训练模型来初始化词向量 $x\in\mathbb{R}^n$。使用维基百科文本，他们的模型通过预测每个单词在其上下文中出现的可能性来学习词向量。与其他基于局部共现的向量空间模型类似，生成的词向量捕获了句法和语义信息。
- 每个词与一个矩阵 $X$ 相关联。在所有的实验中，我们都是用 $X=I+\epsilon$ 来初始化，其中 $\epsilon$ 是一个小的高斯噪声。如果向量维度为 $n$，那么矩阵 $X\in\mathbb{R}^{n{\times}n}$。
- 我们将任意长度为 $m$ 的短语或句子表示为向量矩阵对 $((a,A),\cdots,(m,M))$ 的有序列表，其中每一对都是基于该位置的词检索的。

#### 两个词的组合模型
- 前人的方法，通过 $p=f(a,b,R,K)$ 来计算父节点的向量表示，其中 $a,b$ 是子节点的向量表示，$R$ 是一个已知的先验句法关系，$K$ 是背景知识。$f$ 有很多种可能。
- 本文中的模型的特点是：父节点和子节点的向量表示具有相同的维度。
- 我们希望模型可以不依赖于 $R$ 和 $K$，而通过学习矩阵来得到隐含信息。为此，我们提出如下的组合函数：
$$p=f_{A,B}(a,b)=f(Ba,Ab)=g(W[Ba,Ab]^{\top})$$
其中 $A,B$ 是单词 $a,b$ 对应的矩阵，矩阵 $W\in\mathbb{R}^{n{\times}2n}$ 将向量组合映回 $n$ 维。$g$ 可以是恒等映射，但我们使用一个非线性的激活函数。在激活函数之前也可以引入一个偏置项 $b$。

#### 多个单词和词组的递归组合
- 将两个词的组合模型扩展到多个词的情况。主要思想是将同一个函数 $f$ 应用于解析树中的成对成分。为此，我们需要将一个短语或句子的二进制解析树作为输入，并在每个非叶子节点上计算矩阵。
- 因为维度不变，所以函数 $f$ 可以递归的应用，我们定义如下：
$$P=f_M(A,B)=W_M[A,B]^{\top}$$
其中 $W_M\in\mathbb{R}^{n{\times}2n}$，所以有 $P\in\mathbb{R}^{n{\times}}$。
- 同样的，合并后的 $P$ 可以与其他词继续进行合并。

#### 训练的目标函数
- 基于RNN的模型的优点之一是树的每个节点都有一个分布式向量表示(父向量 $p$ )，也可以看作是描述该短语的特征。我们通过在每个父节点上添加一个简单的softmax分类器来训练这些表示，以预测类别分布。如果有 $K$ 个标签，则 $d\in\mathbb{R}^K$ 是一个 $K$ 维多项分布。
- 对于句子 $s$ 和它的树 $t$，我们的目标函数是其所有节点的交叉熵误差之和 $E(s,t,\theta)$。
- 也可以使用RAE中的方法，用重构误差和交叉熵误差的加权和来定义。

#### 学习过程
- 令 $\theta=(W,W_M,W^{label},L,L_M)$ 是我们模型的参数，$\lambda$ 是一个对所有模型参数都具有正则化的超参数向量。$L$ 和 $L_M$ 是所有词向量和词矩阵的集合。目标函数的梯度如下：
$$\frac{{\partial}J}{{\partial}\theta}=\frac{1}{N}\sum\limits_{(x,t)}\frac{{\partial}E(x,t;\theta)}{{\partial}\theta}+\lambda\theta$$
- 为了计算这个梯度，我们首先自底向上计算所有树节点 $(p_i,P_i)$，然后自顶向下对树中每个节点的softmax分类器求导。
- 虽然目标函数不是凸的，但L-BFGS在实际中会平滑的收敛。

#### 低秩矩阵近似
- 如果所有单词的矩阵表示都是 $n$ 维向量，那么整个模型会变得很大。为了降低参数量，我们用下面的低秩对角近似来表示词矩阵：
$$A=UV+{\rm{diag}}(a)$$
其中 $U\in\mathbb{R}^{n{\times}r}.V\in\mathbb{R}^{r{\times}n},a\in\mathbb{R}^n$，实验中我们令 $r=3$。

## 3.3 2013-RNTN
- 情感树库上的语义组合性递归深度模型</br>
(Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank)
- 由于缺乏大的有标签的语义合成资源和模型，我们引入了斯坦福情感树库，以及一个强大的递归神经张量网络，可以准确地预测在这个新的语料库中出现的组合语义效应。
- 斯坦福情感语料库是第一个具有完全标注的句法分析树的语料库，可以完整地分析语言中情感的组合效应。
- 斯坦福情感树库包含了成千上万句子中每个句法上合理的短语的标签，允许我们训练和评估组合模型。本文考虑电影评论语料库，其中一半积极一半消极。使用人工标注，标签分为5类。

### 3.3.1 递归神经模型
- 每个单词表示为一个 $d$ 维向量。从一个均匀分布 $\mathcal{U}(-r,r)$ 中采样进行初始化，其中 $r=0.0001$。
- 堆叠词向量得到词嵌入矩阵 $L\in\mathbb{R}^{d{\times}|V|}$，其中 $|V|$ 是词汇表的大小。$L$ 是与模型联合训练的参数矩阵。
- 我们可以将词向量作为参数，输入一个softmax分类器来训练，将其分为5个类别，我们通过下式计算其后验概率：
$$y^a={\rm{softmax}}(W_sa)$$
其中 $W_s\in\mathbb{R}^{5{\times}d}$ 是情感分类矩阵。
- 对于给定的三元组，对向量b和c重复此过程。模型的主要任务和区别在于自底向上地计算隐变量 $p_i\in\mathbb{R}^d$。

#### 递归神经网络
- RNN通过下式递归的计算父节点的向量表示：
$$p_1=f(W[b,c]^{\top}),p_2=f(W[a,p_1]^{\top})$$
其中 $f$ 是一个非线性激活函数，$W\in\mathbb{R}^{d{\times}2d}$ 是可学习的参数矩阵，我们省略了偏置项。每个父节点通过相同的softmax分类器来学习标签的概率。
- 模型使用相同的语义合成函数作为递归自编码器。和以前的模型不同的是，我们固定了树的结构，并且忽略了重构误差。

#### MV-RNN：矩阵-向量RNN
- MV-RNN的主要思想是将解析树中的每个词和更长的短语表示为向量和矩阵。当两个词组合时，一个词的矩阵与另一个词的向量相乘，反之亦然。
- 其递归算式如下：
$$p_1=f(W[Cb,Bc]^{\top}),P_1=f(W_M[B,C]^{\top})$$
其中 $W_M\in\mathbb{R}^{d{\times}2d}$，$P_1$ 仍是 $d{\times}d$ 维的。
- 继续递归的计算 $(p_1,P_1)$ 和 $(a,A)$。同样使用相同的softmax分类器来学习分布。

#### RNTN：递归神经张量网络
- 下图展示了一个张量层的结构：

![](./img/3.3.1RNTN.png ':size=75%')
- 我们定义张量积 $h\in\mathbb{R}^d$ 运算的输出为：
$$h=[b,c]V^{[1:d]}[b,c]^{\top};h_i=[b,c]V^{[i]}[b,c]^{\top}$$
其中 $V^{[1:d]}\in\mathbb{R}^{2d{\times}2d{\times}d}$ 定义了多个双线性形张量。
- RNTN通过下式计算 $p_1$：
$$p_1=f\left([b,c]V^{[1:d]}[b,c]^{\top}+W[b,c]^{\top}\right)$$
其中 $W$ 是之前定义的权重矩阵。
- 以同样的权重矩阵继续递归计算：
$$p_2=f\left([a,p_1]V^{[1:d]}[a,p_1]^{\top}+W[a,p_1]^{\top}\right)$$

#### 结构张量反向传播
- 如前所述，每个节点都有一个在其向量表示上训练的softmax分类器来预测给定的真实值或目标向量 $t$。我们假设每个节点上的目标分布向量具有 $0-1$ 独热编码。
- 对于节点 $i$ 的预测分布 $y^i\in\mathbb{R}^{C{\times}1}$ 和目标分布 $t^i\in\mathbb{R}^{C{\times}1}$，我们最大化正确预测的概率，或者最小化交叉熵损失，等价于最小化两个分布的KL散度。
- RNTN参数 $\theta(V,W,W_s,L)$ 对应的损失函数为：
$$E(\theta)=\sum\limits_i\sum\limits_jt_j^i{\rm{log}}y_j^i+\lambda\|\theta\|^2$$
- 定义 $x_i$ 是节点 $i$ 的向量，$\delta^{i,s}\in\mathbb{R}^{d{\times}1}$ 是节点 $i$ 的softmax误差向量：
$$\delta^{i,s}=(W_s^{\top}(y^i-t^i)){\otimes}f^{\prime}(x^i)$$
其中 $\otimes$ 是哈达玛积，$f^\prime$ 是 $f$ 的导数。
- 其余的导数只能以自顶向下的方式从树的顶点到叶节点进行计算。$V$ 和 $W$ 的全导数为各节点处的导数之和。
- 我们以下图的结构举例：

![](./img/3.3.2RNTN.png ':size=40%')
- 我们定义节点 $i$ 的传入误差为 $\delta^{i,com}$，顶点的传入误差仅由顶点的softmax函数得到，即 $\delta^{p_2,com}=\delta^{p_2,s}$。对每项求导，我们有：
$$\frac{{\partial}E^{p_2}}{{\partial}V^{[k]}}=\delta_k^{p_2,com}[a,p_1]^{\top}[a,p_1]$$
其中 $\delta_k^{p_2,com}$ 是向量 $\delta^{p_2,com}$ 的第 $k$ 个元素。
- 我们通过 $p_2$ 的孩子节点计算误差信息：
$$\delta^{p_2,down}=\left(W^{\top}\delta^{p_2,com}+S\right){\otimes}f^{\prime}([a,p_1]^{\top})$$
其中 $S=\sum\limits_{k=1}^d\delta_k^{p_2,com}\left(V^{[k]}+(V^{[k]})^{\top}\right)[a,p_1]^{\top}$
- $p_2$ 的孩子节点取这个向量的一半并添加自己的softmax误差得到完整的 $\delta$。例如下式：
$$\delta^{p_1,com}=\delta^{p_1,s}+\delta^{p_2,down}[d+1:2d]$$
其中 $\delta^{p_2,down}[d+1:2d]$ 暗示了 $p_1$ 为 $p_2$ 的右孩子。对于左孩子 $a$，这一项为 $\delta^{p_2,down}[1:d]$。
- 例子中的三元组的全导数可以表示为：
$$\frac{{\partial}E}{{\partial}V^{[k]}}=\frac{E^{p_2}}{{\partial}V^{[k]}}+\delta_k^{p_1,com}[b,c]^{\top}[b.c]$$
$W$ 的全导数与之类似。
- 我们使用AdaGrad优化器在3小时内收敛到局部最优。

## 3.4 2014-GBRNN
- 全局信念递归神经网络</br>
(Global Belief Recursive Neural Networks)
- 我们引入一个新的模型来计算上下文相关的可变长度短语的合成向量表示。
#### 上下文相关作为全局信念的动机
- 在将句子映射为特征向量时，一个常见的简化假设是不考虑词序。然而，这将阻止对语言的任何详细理解，例如 "Android胜过iOS" 这个短语的整体情绪是不清楚的。
- 相反，我们需要对每个短语的理解，这将引导我们进入深度递归模型。将句子映射到向量空间的第一步是将其解析为二叉树结构，以捕获单词之间的语法关系。这样一个输入相关的二叉树决定了递归神经网络的结构，它将以自底向上的方式从词向量开始计算隐藏向量。生成的短语向量作为特征提供给分类器。这种标准的RDL架构很好地对短语固有的或上下文无关的标签进行了分类。例如，它可以正确地分类 "一个不那么美丽的日子" 是负面的情绪。然而，并非所有的短语都具有内在情感，如 "Android胜过iOS" 的例子。
- GB-RNN通过将信息从根节点传播回叶节点来解决这个问题。还有其他方法可以结合上下文，如双向递归神经网络或基于窗口的方法。然而，这两种方法都不能从离待标注短语更远的词语中提取信息。

#### 标准递归神经网络
- 假定每一个词向量 $a\in\mathbb{R}^n$ 从一个均匀分布中采样初始化 $a_i{\sim}\mathcal{U}(-0.001,0.001)$。这些向量构成一个嵌入矩阵 $L\in\mathbb{R}^{n{\times}|V|}$，其中 $|V|$ 是词汇表的大小。所有的词向量由模型一起学习。

![](./img/3.4.1GB-RNN.png ':size=100%')
- 如上图的例子中，RNN方程定义如下：
$$p_1=f(W[b,c]^{\top}),\ p_2=f(W[a,p_1]^{\top})$$
其中 $W\in\mathbb{R}^{n{\times}2n}$ 是参数矩阵，$f$ 是非线性激活函数。
- 每个节点向量由一个softmax分类器给出。

#### GB-RNN
- 我们的目标是在递归过程中包含上下文信息。一个简单的解决方法是：只在每对词的左边和右边包含k个上下文词。然而，上下文的视野是有限的，并且为了捕捉更复杂的语言现象，也许有必要允许多个词语组成语境意义的转换。
- 我们将使用标准RNN架构中的前馈节点，并简单地向下移动树。这也可以理解为树的展开或树枝的上移。
- 因此，我们保持相同的方程计算前向节点向量，但在解析树的每一层引入新的反馈向量，用向下箭头 $\downarrow$ 表示。不同于前馈向量采用自下而上的方式计算，反馈向量采用自上而下的递归函数计算。
- 在图例中的顶点 $p_2$，我们有：
$$p_2^{\downarrow}=f(Vp_2)$$
其中 $V\in\mathbb{V}^{n_d{\times}n}$，使得所有的反馈向量是 $n_d$ 维的。
- 继续递归计算，我们有：
$$[a^{\downarrow},p_1^{\downarrow}]^{\top}=f\left(W^{\downarrow}[p_2,p_2^{\downarrow}]^{\top}\right),[b^{\downarrow},c^{\downarrow}]^{\top}=f\left(W^{\downarrow}[p_1,p_1^{\downarrow}]^{\top}\right)$$
其中所有的反馈向量是 $n_d$ 维的，所以 $W^{\downarrow}\in\mathbb{R}^{(n+n_d){\times}(n+n_d)}$。
- 计算完前向和后向的两个向量后，我们简单的进行concat操作，然后放进标准的softmax分类器中得到最后的预测，例如单词 $a$ 的分类为：
$$y_a={\rm{softmax}}(W_c[a,a^{\downarrow}])$$
其中 $C$ 类分类器的权重矩阵 $W_c\in\mathbb{R}^{C{\times}(n+1)}$。
- 根节点的方程 $x_{root}^{\downarrow}$ 可以用 $x_{root}$ 替代。但引入变换矩阵 $V$ 有两个优点，它有助于清晰地区分前向步骤和后向步骤计算的特征，并且允许对 $x^{\downarrow}$ 向量使用不同的维度，减少了 $W^{\downarrow}$ 和 $W_{class}$ 矩阵中的参数个数，为模型增加了更多的灵活性。实践中也表现得更好。

#### 混合词向量表示
- RNN模型词向量有两种初始化方式。
    - 最简单的方法是均匀分布随机初始化，这样做的优点是不需要任何预训练方法，并且向量一定能够捕获领域知识。然而，向量更容易过拟合，不太能泛化到训练集之外的单词。
    - 另一种方法是使用无监督的方法学习语义词向量来预训练。但是减慢了训练，增加了大量的参数。
- 在本文中，我们将这两种思想结合起来，将无监督预训练向量和我们的传播任务特定的误差concat到一起。

#### 训练
- GB-RNN使用反向传播来训练。通过小批量 AdaGrad 优化器优化交叉熵损失。我们使用50%概率置0的Dropout来提升模型的性能。
- 我们对权重矩阵正则化、词向量大小、mini-batch大小、dropout概率和激活函数进行了交叉验证。
