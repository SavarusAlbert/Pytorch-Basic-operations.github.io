# ReNN系列论文泛读
## 3.1 RAE
- 用于预测情感分布的半监督递归自编码器(2011)</br>
(Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions)
### 3.1.1 任务目的
- RAE目的解决以下三个问题：
    - 模型利用分层结构和组合语义学来替代词袋表示，从而完成情感分类任务；
    - 在无标签数据或有监督的情感数据集上，不需要特定领域的情感词典、语法程序等条件，模型也能够训练；
    - 对于多种复杂的、相互关联的情感，我们预测了一个多维度的分布，而不是仅仅限制在积极/消极。
### 3.1.2 半监督递归自编码器
#### 神经网络词表示
- 我们将单词表示为一个带参数的连续向量，做如下两个设置：
    - 从一个0均值高斯分布中采样来进行初始化：
    $$x{\sim}\mathcal{N}(0,\sigma^2),x\in\mathbb{R}^n$$
    词向量组成一个词嵌入矩阵( $|V|$ 是词汇表的大小)：
    $$L\in\mathbb{R}^{n{\times}|V|}$$
    - 通过一个无监督的神经网络语言模型来进行预训练来学习一个词嵌入，从共现统计学中捕获句法和语义信息。
- 词向量 $x_i$ 可由词嵌入矩阵 $L$ 和一个独热向量 $b$ 乘积得到：
$$x_i=Lb_k\in\mathbb{R}^n$$
- 我们用有序列表 $(x_1,\cdots,x_m)$ 来表示一个句子(或n-gram)。

#### 传统递归自编码器

![](./img/3.1.1RAE.png ':size=50%')
- 以上图举例，给定一个先验的树结构，传统的递归自编码器通过在树结构上构建维度不变的函数变换，来递归的计算子节点 $c_1$ 和 $c_2$ 的父节点 $p$：
$$p=f(W^{(1)}[c_1;c_2]+b^{(1)})$$
其中 $W^{(1)}$ 是权重矩阵，$b^{(1)}$ 是偏置。然后引入一个逐元素的激活函数，来引入非线性。
- 通过构建一个重构层，来评估模型提取词嵌入的能力：
$$[c_1^{\prime};c_2^{\prime}]=W^{(2)}p+b^{(2)}$$
- 训练目标是最小化重构误差，例如欧式距离：
$$E_{rec}([c_1;c_2])=\frac{1}{2}\left\|[c_1;c_2]-[c_1^{\prime};c_2^{\prime}]\right\|^2$$

#### 为了结构预测的无监督递归自编码器
- 假定对于输入向量 $x$，没有先验的树结构。我们定义由句子 $x$ 生成的所有可能的树结构的集合为 $A(x)$。令 $T(y)$ 是由非叶子节点 $s$ 组成的三元组，使用欧式距离计算重构误差，我们有：
$${\rm{RAE}}_\theta(x)=\mathop{\rm{argmin}}\limits_{y{\in}A(x)}\sum\limits_{s{\in}T(y)}E_{rec}([c_1;c_2]_s)$$
###### 贪心无监督RAE
- 由 $m$ 个单词组成的句子，从句首开始，定义临近的两个词 $(c_1;c_2)=(x_1;x_2)$ 组成的词组为潜在父节点，我们存储可能的父节点 $p$，以及重构误差。
- 计算好后移动一格继续计算 $(c_1;c_2)=(x_2;x_3)$，直到句子结尾 $(c_1;c_2)=(x_{m-1};x_m)$。用最小的重构误差以及对应的潜在父节点来贪心的定义新句子。
- 举例来说，假定句子 $(x_1,x_2,x_3,x_4)$ 的最小重构误差为单词对 $(x_3;x_4)$，那么新句子为 $(x_1,x_2,p_{(3,4)})$。继续递归可能得到 $(x_1,p_{(2,(3,4))})$ 或 $(p_{(1,2)},p_{(3,4)})$。

###### 权重重构
- 词组的单词数不同，我们更希望对词组分配更多的关注，为此我们给重构误差分配权重：
$$E_{rec}([c_1;c_2];\theta)=\frac{n_1}{n_1+n_2}\|c_1-c_1^{\prime}\|^2+\frac{n_2}{n_1+n_2}\|c_2-c_2^{\prime}\|^2$$
其中 $[n_1;n_2]$ 是 $[c_1;c_2]$ 的单词数。

###### 长度归一化
- RAE对词组和词在树中的高度来最小化重构误差，由于其计算隐藏层参数后重构误差，RAE可以通过仅仅降低参数大小来降低误差，这是我们不希望发生的情况。
- 因此调整隐藏层来使得父节点 $p$ 的向量表示模长为1：
$$p=\frac{p}{\|p\|}$$

#### 半监督递归自编码器
- RAE的主要优点之一是，由RAE构建的树的每个节点都有一个分布式向量表示，它也可以被看作描述该短语的特征。我们可以利用这种表示方式，在每个父节点的顶部添加一个简单的softmax层来预测分类的分布：
$$d(p;\theta)={\rm{softmax}}(W^{label}p)$$
- 假定有 $K$ 个标签，$d\in\mathbb{R}^K$ 是一个 $K$ 维多项分布，且有 $\sum_{k=1}^Kd_k=1$。
- 下图展示了这样的半监督RAE单元：

![](./img/3.1.2RAE.png ':size=60%')
- 我们的分类器的目标是一个多项标签分布 $t$，第 $k$ 项用 $t_k$ 来表示，softmax输出一个条件分布 $d_k=p(k|[c_1;c_2])$，因此交叉熵损失为($-p{\rm{log}}q$)：
$$E_{cE}(p,t;\theta)=-\sum\limits_{k=1}^Kt_k{\rm{log}}d_k(p;\theta)$$
- 最终我们的RAE预测句子和标签的组合，通过优化下式来训练：
$$J=\frac{1}{N}\sum\limits_{(x,t)}E(x,t;\theta)+\frac{\lambda}{2}\|\theta\|^2$$
其中第一部分由贪心算法来求得：
$$E(x,t;\theta)=\sum\limits_{s{\in}T(RAE_\theta(x))}E([c_1;c_2]_s,p_s,t,\theta)$$
每个非叶子节点处的误差为重构误差和交叉熵误差的加权和：
$$E([c_1;c_2]_s,p_s,t,\theta)={\alpha}E_{rec}([c_1;c_2]_s;\theta)+(1-\alpha)E_{cE}(p_s,t;\theta)$$
其中 $\alpha$ 是超参数。
- 为了使用该模型预测句子的情感分布，我们使用学习到的树节点的向量表示训练一个简单的逻辑回归分类器。
### 训练
- 令 $\theta=(W^{(1)},b^{(1)},W^{(2)},b^{(2)},W^{label},L),$ 是模型参数，那么梯度为：
$$\frac{{\partial}J}{{\partial}\theta}=\frac{1}{N}\sum\limits_{(x,t)}\frac{{\partial}E(x,t;\theta)}{{\partial}\theta}+\lambda\theta$$
- 由于我们使用贪心算法构造树结构，在计算梯度的时候就不一定是连续的，在梯度下降的方向上不一定会降低目标函数。
- 然而，我们发现L-BFGS在完整的训练数据(批处理模式)上运行以最小化目标在实际中效果很好，并且收敛是平滑的，算法通常能快速找到一个好的解。


## 3.2 MV-RNN
- 通过递归的矩阵-向量空间的语义组合性(2012)</br>
(Semantic Compositionality through Recursive Matrix-Vector Spaces)
### 3.2.1 MV-RNN 递归矩阵-向量模型
- 从单个词向量表示构建词组表示的主要方法是将词组表示成单个词向量的线性组合，如求和或加权平均。当词组恰好为其各个部分意义加一起时有效，但如果是一个词修饰另一个词的情况就会失效。
- 一种可能的方法是通过MLP来构造词组的表示：
$$p=g(W[a,b]^{\top})$$
其中 $p$ 是父节点，$g$ 是激活函数，$W$ 是权重矩阵。然后通过构造二叉树来应用上述方法递归计算。但通过一个相同的 $W$ 很难同时让所有词组的向量表示成立。
- 另一种通过矩阵建模来捕获语义向量空间中的信息。但这些方法仅限于捕获单词对的线性函数，而我们希望通过非线性函数来计算多词短语或完整句子的组合意义表示。
- MV-RNN吸取了这些方法的长处：
    - 对每个单词指定一个向量和一个矩阵
    - 学习一个特定输入的、非线性的复合函数，用于计算任意句法类型的多词序列的向量和矩阵表示。

#### 矩阵-向量神经网络词表示
- 我们通过一个50维词向量预训练模型来初始化词向量 $x\in\mathbb{R}^n$。使用维基百科文本，他们的模型通过预测每个单词在其上下文中出现的可能性来学习词向量。与其他基于局部共现的向量空间模型类似，生成的词向量捕获了句法和语义信息。
- 每个词与一个矩阵 $X$ 相关联。在所有的实验中，我们都是用 $X=I+\epsilon$ 来初始化，其中 $\epsilon$ 是一个小的高斯噪声。如果向量维度为 $n$，那么矩阵 $X\in\mathbb{R}^{n{\times}n}$。
- 我们将任意长度为 $m$ 的短语或句子表示为向量矩阵对 $((a,A),\cdots,(m,M))$ 的有序列表，其中每一对都是基于该位置的词检索的。

#### 两个词的组合模型
- 前人的方法，通过 $p=f(a,b,R,K)$ 来计算父节点的向量表示，其中 $a,b$ 是子节点的向量表示，$R$ 是一个已知的先验句法关系，$K$ 是背景知识。$f$ 有很多种可能。
- 本文中的模型的特点是：父节点和子节点的向量表示具有相同的维度。
- 我们希望模型可以不依赖于 $R$ 和 $K$，而通过学习矩阵来得到隐含信息。为此，我们提出如下的组合函数：
$$p=f_{A,B}(a,b)=f(Ba,Ab)=g(W[Ba,Ab]^{\top})$$
其中 $A,B$ 是单词 $a,b$ 对应的矩阵，矩阵 $W\in\mathbb{R}^{n{\times}2n}$ 将向量组合映回 $n$ 维。$g$ 可以是恒等映射，但我们使用一个非线性的激活函数。在激活函数之前也可以引入一个偏置项 $b$。

#### 多个单词和词组的递归组合
- 将两个词的组合模型扩展到多个词的情况。主要思想是将同一个函数 $f$ 应用于解析树中的成对成分。为此，我们需要将一个短语或句子的二进制解析树作为输入，并在每个非叶子节点上计算矩阵。
- 因为维度不变，所以函数 $f$ 可以递归的应用，我们定义如下：
$$P=f_M(A,B)=W_M[A,B]^{\top}$$
其中 $W_M\in\mathbb{R}^{n{\times}2n}$，所以有 $P\in\mathbb{R}^{n{\times}}$。
- 同样的，合并后的 $P$ 可以与其他词继续进行合并。

#### 训练的目标函数
- 基于RNN的模型的优点之一是树的每个节点都有一个分布式向量表示(父向量 $p$ )，也可以看作是描述该短语的特征。我们通过在每个父节点上添加一个简单的softmax分类器来训练这些表示，以预测类别分布。如果有 $K$ 个标签，则 $d\in\mathbb{R}^K$ 是一个 $K$ 维多项分布。
- 对于句子 $s$ 和它的树 $t$，我们的目标函数是其所有节点的交叉熵误差之和 $E(s,t,\theta)$。
- 也可以使用RAE中的方法，用重构误差和交叉熵误差的加权和来定义。

#### 学习过程
- 令 $\theta=(W,W_M,W^{label},L,L_M)$ 是我们模型的参数，$\lambda$ 是一个对所有模型参数都具有正则化的超参数向量。$L$ 和 $L_M$ 是所有词向量和词矩阵的集合。目标函数的梯度如下：
$$\frac{{\partial}J}{{\partial}\theta}=\frac{1}{N}\sum\limits_{(x,t)}\frac{{\partial}E(x,t;\theta)}{{\partial}\theta}+\lambda\theta$$
- 为了计算这个梯度，我们首先自底向上计算所有树节点 $(p_i,P_i)$，然后自顶向下对树中每个节点的softmax分类器求导。
- 虽然目标函数不是凸的，但L-BFGS在实际中会平滑的收敛。

#### 低秩矩阵近似
- 如果所有单词的矩阵表示都是 $n$ 维向量，那么整个模型会变得很大。为了降低参数量，我们用下面的低秩对角近似来表示词矩阵：
$$A=UV+{\rm{diag}}(a)$$
其中 $U\in\mathbb{R}^{n{\times}r}.V\in\mathbb{R}^{r{\times}n},a\in\mathbb{R}^n$，实验中我们令 $r=3$。

## 3.3 RNTN