# 语言模型
## 1.1 语言模型概述
### 1.1.1 自然语言处理
- 自然语言处理(Natural Language Processing)是人工智能和语言学领域的分支学科，探讨如何处理及运用自然语言。
- 自然语言处理主要应用于机器翻译、舆情监测、自动摘要、观点提取、文本分类、问题回答、文本语义对比、语音识别、中文OCR等方面。
### 1.1.2 语言模型
- 语言模型定义了自然语言中标记序列的概率分布。根据模型的设计，标记可以是词、字符、甚至是字节。标记总是离散的实体。
- 语言模型的发展可以分为两个大的阶段，分别是使用n-gram语言模型的统计语言模型阶段，和后来居上的神经网络语言模型阶段。
- 神经网络语言模型又可以分为早期的以设计神经网络结构为主的阶段和目前的通过巨量语料进行训练，再通过微调进行部署的预训练语言模型阶段。

## 1.2 语言模型任务分类及主流算法
#### 文本分类任务
- 主要算法：Fasttext；ReNN；MLP；RNN；CNN；Attention；Transformer；GNN

#### 句间匹配
- 主要算法：Representation based；interaction-based

#### 序列标注
- 主要算法：Embedding Module；Context Encoder Module；Inference Module

#### 文本生成

#### 语言模型
- 主要算法：word level；sentence level

## 1.3 下游任务分类
#### 单一任务
- 词法分析：分词；词性识别；NER
- 句法分析：依存分析；语义角色标注
- 语义分析：情感分析；意图识别；信息抽取；同义识别；指代消解；阅读理解；文本纠错
- 文本生成：生成式摘要；机器翻译；对话问答
#### 复杂任务
- 搜索、推荐；对话；知识图谱

## 1.4 统计语言模型
- 基于 $n-{\rm{gram}}$ 的模型定义了一个条件概率：给定前 $n-1$ 个标记后的第 $n$ 个标记的条件概率。
- 使用这些条件概率的分布的乘积定义较长序列的概率分布：
$$P(x_1,\cdots,x_\tau)=P(x_1,\cdots,x_{n-1})\prod\limits_{t=n}^{\tau}P(x_t|x_{t-n+1},\cdots,x_{t-1})$$
其中，初始序列 $P(x_1,\cdots,x_{n-1})$ 的概率分布可以通过带有较小 $n$ 值的不同模型建模。
- 当句子较长时，参数呈指数级增长，为了解决这个问题，引入马尔可夫假设：当前词的出现概率仅与前 $n−1$ 个词有关。
- $n-{\rm{gram}}$ 模型最大似然的基本限制是，在许多情况下从训练集计数估计得到的 $P_n$ 很可能为0。导致概率为0，或者对数似然趋于 $-\infty$。
    - 为解决这一问题，引入了数据平滑的方法，其基本思想为调整最大似然估计的概率值，使零概率增值，使非零概率下调，“劫富济贫”，消除零概率，改进模型的整体正确率。具体方法有Good-Turing法，Katz后退法，绝对减值法，线性减值法等。
- 为了提高 $n-{\rm{gram}}$ 模型的统计效率，基于类的语言模型(class-based language model)引入词类别的概念，然后属于同一类别的词共享词之间的统计强度。

## 1.5 神经网络语言模型
- 神经语言模型是一类用来客服维数灾难的语言模型，它使用词的分布式表示对自然语言序列建模。
- 神经语言模型将词以抽象符号的表示方法转变为了语义空间下的向量表示(词嵌入)，再以向量表示的单词序列作为神经网络的输入，来求词的概率。
### 1.5.1 序列标注
- 主要算法：
    - Bi-LSTM+CRF
    - ID-CNN+CFR
    - ELMO+Bi-LSTM+CFR
    - CVT+Multi-task+BERT

### 1.5.2 文本分类
- 主要算法：
    - FastText
    - CNN：TextCNN、VeryDeep CNN、GCNN
    - RNN：TextRNN、TextLSTM、Attention
    - 基于上下文的LM：BERT、EMLo、GPT、ULMFiT

### 1.5.3 文本匹配
- 主要算法：
    - Siamese
    - Interaction

### 1.5.4 语言模型
- 主要算法：
    - BERT：BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding
    - GPT