# Chapter01 生成模型
## 1.1 生成模型概述
- 在统计分类问题中，主要的两种方法分别是生成方法和判别方法。对应到实际的模型可以分为以下两类：
    - 生成模型(generative model)：给定目标 $y$ ，求可观测的 $X$ 的条件概率 $P(X|Y=y)$。
    - 判别模型(discriminative model)：对于给定的观测量 $x$ ，求目标 $Y$ 的条件概率 $P(Y|X=x)$。
- 在实际应用中，可观测集 $X$ 通常来说是一组连续变量，目标 $Y$ 通常是由一组有限的标签组成的离散变量。条件概率 $P(Y|X)$ 也可以被表示为求目标函数 $f:X{\rightarrow}Y$。
### 1.1.1 一些常用的概率关系
- 给定一个联合概率分布 $P(X,Y)$，将 $X$ 视为连续变量，$Y$ 视为离散变量，我们有：
$$P(X)=\sum\limits_yP(X,Y=y)$$
$$P(Y)=\int_xP(Y,X=x)$$
- 条件概率公式：
$$P(X|Y)=\frac{P(X,Y)}{P(Y)},{\quad}P(Y|X)=\frac{P(X,Y)}{P(X)}$$
- 贝叶斯公式：
$$P(X|Y)P(Y)=P(Y|X)P(X)$$

### 1.1.2 生成模型分类
生成模型的类型主要包括：
- 高斯混合模型(Gaussian mixture model)
- 隐马尔可夫模型(Hidden Markov model)
- 概率上下文无关语法(Probabilistic context-free grammar)
- 贝叶斯网络(Bayesian network)
- 平均一依赖性估计器(Averaged one-dependence estimators)
- 潜在狄利克雷分配(Latent Dirichlet allocation)
- 玻尔兹曼机(Boltzmann machine)
- 变分自动编码器(Variational autoencoder)
- 生成对抗网络(Generative adversarial network)
- 基于流的生成模型(Flow-based generative model)
- 基于能量的模型(Energy based model)
- 扩散模型(Diffusion model)

### 1.1.3 判别模型分类
判别模型的类型主要包括：
- k-最近邻算法(k-nearest neighbors algorithm)
- 逻辑回归(Logistic regression)
- 支持向量机(Support Vector Machines)
- 决策树学习(Decision Tree Learning)
- 随机森林(Random Forest)
- 最大熵马尔可夫模型(Maximum-entropy Markov models)
- 条件随机场(Conditional random fields)

## 1.2 基于能量的模型
- 无向图模型的很多结论都依赖于假设：$\forall{x},\tilde{p}(x)>0$。使这个条件满足的一种简单方式是使用基于能量的模型：
$$\tilde{p}(x)={\rm{exp}}(-E(x))$$
$E(x)$ 被称作能量函数(energy function)，上式保证了 $x$ 的概率始终大于0，所以我们可以自由的选择那些能够简化学习过程的能量函数。
- 服从能量函数的任意分布都是玻尔兹曼分布的一个实例。
- 玻尔兹曼分布是一种概率分布模型，描述了某种状态的概率和其能量、系统温度之间的关系：
$$p_i=\frac{1}{Q}e^{-\epsilon_i/(kT)}=\frac{e^{-\epsilon_i/(kT)}}{\sum_{j=1}^Me^{-\epsilon_i/(kT)}}$$
其中，$p_i$ 是状态 $i$ 的概率，$\epsilon_i$ 是状态 $i$ 的能量，$k$ 是玻尔兹曼常数，$T$ 是系统的绝对温度，$Q$ 是归一化函数，$Q=\sum\limits_{i=1}^Me^{-\epsilon_i/(kT)}$。

### 1.2.1 玻尔兹曼机(Boltzmann machine)
原始玻尔兹曼机用来学习二值向量上的任意概率分布。我们在 $d$ 维二值随机向量 $x\in\{0,1\}^d$ 上定义玻尔兹曼机。
- 玻尔兹曼机是一种基于能量的模型，意味着我们可以使用能量函数定义联合概率分布：
$$p(x)=\frac{{\rm{exp}}(-E(x))}{Z}$$
其中 $e(x)$ 是能量函数，$Z$ 是确保 $\sum_xP(x)=1$ 的配分函数。
- 玻尔兹曼机的能量函数如下给出：
$$E(x)=-x^{\top}Ux-b^{\top}x$$
其中 $U$ 是模型参数的"权重"矩阵，$b$ 是偏置向量。
- 玻尔兹曼机的能量函数描述了观察到的变量的联合概率分布。但它限制了观察到的变量和权重矩阵描述的变量之间相互作用的类型。具体来说，这意味着一个单元的概率由其他单元值的线性模型(逻辑回归)给出。与MLP中添加隐藏单元的想法类似，具有隐藏单元的玻尔兹曼机不再局限于建模变量之间的线性关系。相反，玻尔兹曼机变成了离散变量上概率质量函数的万能近似器。
- 我们将单元 $x$ 分解为两个子集：可见单元 $v$ 和潜在(或隐藏)单元 $h$。能量函数变为：
$$E(v,h)=-v^{\top}Rv-v^{\top}Wh-h^{\top}Sh-b^{\top}v-c^{\top}h$$

### 1.2.2 受限玻尔兹曼机(RBM)
- RBM包含两个层，可见层(visible layer)和隐藏层(hidden layer)。神经元之间的连接具有如下特点：层内无连接，层间全连接(双向连接且同一个连接的每个方向同权重)，显然RBM对应的图是一个二分图。

![](./img/1.2RBM.png ':size=40%')
- 令可见层由一组 $n_v$ 个二值随机变量组成，我们统称为向量 ${\rm{v}}$，将 $n_h$ 个二值随机变量的潜在或隐藏层记为 ${\rm{h}}$。因为连接为全连接，因此两层的地位是相同的。联合概率分布由能量函数指定：
$$P({\rm{v}}=v,{\rm{h}}=h)=\frac{1}{Z}{\rm{exp}}(-E(v,h))$$
其中 $Z$ 是被称为配分函数的归一化常数，$Z=\sum\limits_v\sum\limits_h{\rm{exp}}\{-E(v,h)\}$。
- RBM的能量函数由下给出：
$$E(v,h)=-b^{\top}v-c^{\top}h-v^{\top}Wh$$
- $P(v)$ 是难解的，但RBM的二分图结构具有非常特殊的性质，每层没有层内的连接，所以其条件分布 $P(h|v)$ 和 $P(v|h)$ 是因子的，并且计算和采样是相对简单的。关于隐藏层的完全条件分布表达为：
$$P(h|v)=\prod\limits_{j=1}^{n_h}\sigma\left((2h-1)\odot(c+W^{\top}v)\right)_j$$
$P(v|h)$ 也是因子形式的分布：
$$P(v|h)=\prod\limits_{i=1}^{n_v}\sigma\left((2v-1)\odot(b+Wh)\right)_i$$

### 1.2.3 深度信念网络(DBN)
- 深度信念网络是具有若干潜变量层的生成模型。顶部两层之间的连接是无向的。而其他多层之间的连接是有向的。

![](./img/1.2DBN.png ':size=40%')
- 上图中的三层的DBN结构可以理解为：固定住可见层的 ${\rm{v}}$，通过两层RBM去学习隐藏层 ${\rm{h}}$，由于我们要固定可见层，通过叠加RBM来学习前面层的权重，因此最后一层的连接是单向的，为了让最后一层不会影响上面层的学习。如果是多层RBM，则除了最上面是无向的，后续每层之间的连接都是单向的。只有一个隐藏层的DBN只是一个RBM。
- 具有 $l$ 个隐藏层的DBN包含 $l$ 个权重矩阵：$W^{(1)},\cdots,W^{(l)}$。同时也包含 $l+1$ 个偏置向量：$b^{(0)},\cdots,b^{(l)}$，其中 $b^{(0)}$ 是可见层的偏置。DBN表示的概率分布由下式给出：
$$P(h^{(l)},h^{(l-1)})\propto{\rm{exp}}({b^{(l)}}^{\top}h^{(l)}+{b^{(l-1)}}^{\top}h^{(l-1)}+{h^{(l-1)}}^{\top}W^{(l)}h^{(l)})$$
$$P(h_i^{(k)}=1|h^{(k+1)})=\sigma(b_i^{(k)}+{W_{:,i}^{(k+1)}}^{\top}h^{(k+1)}),{\forall}i,{\forall}k{\in}1,\cdots,l-2$$
$$P(v_i=1|h^{(l)})=\sigma(b_i^{(0)}+{W_{:,i}^{(1)}}^{\top}h^{(1)}),{\forall}i$$
在实值可见单元的情况下，替换
$${\rm{v}}\sim\mathcal{N}(v;b^{(0)}+{W^{(1)}}^{\top}h^{(1)},\beta^{-1})$$
- 深度信念网络中的推断是难解的，评估或最大化对数似然的标准证据下界也是难以处理的。为训练深度信念网络，我们可以先使用对比散度或随机最大似然方法训练RBM以最大化
$$\mathbb{E}_{{\rm{v}}{\sim}p_{\rm{data}}}{\rm{log}}p(v)$$
然后，第二个RBM训练为近似最大化
$$\mathbb{E}_{{\rm{v}}{\sim}p_{\rm{data}}}\mathbb{E}_{{\rm{h}}^{(1)}{\sim}p^{(1)}(h^{(1)}|v)}{\rm{log}}p^{(2)}(h^{(1)})$$
其中 $p^{(i)}$ 是第 $i$ 个RBM表示的概率分布。
- 第二个 RBM 被训练为模拟由第一个 RBM 的隐藏单元采样定义的分布，而第一个 RBM 由数据驱动。这个过程能无限重复，从而向 DBN 添加任意多层，其中每个新的 RBM 对前一个 RBM 的样本建模。每个 RBM 定义 DBN 的另一层。这个过程可以被视为提高数据在 DBN 下似然概率的变分下界。

### 1.2.4 深度玻尔兹曼机(RBM)
- 深度玻尔兹曼机是另一种深度生成模型，与深度信念网络(DBN)不同的是，它是一个完全无向的模型。每一层内的每个变量是相互独立的，并条件于相邻层中的变量。

![](./img/1.2DBM.png ':size=40%')
- 在一个深度玻尔兹曼机包含一个可见层 $v$ 和三个隐藏层 $h^{(1)}$，$h^{(2)}$ 和 $h^{(3)}$ 的情况下，联合概率由下式给出：
$$P(v,h^{(1)},h^{(2)},h^{(3)})=\frac{1}{Z(\theta)}{\rm{exp}}\left(-E(v,h^{(1)},h^{(2)},h^{(3)};\theta)\right)$$
- DBM 能量函数定义如下(省略了偏置参数)：
$$E(v,h^{(1)},h^{(2)},h^{(3)};\theta)=-v^{\top}W^{(1)}h^{(1)}-{h^{(1)}}^{\top}W^{(2)}h^{(2)}-{h^{(2)}}^{\top}W^{(3)}h^{(3)}$$
- DBM 的层可以组织成一个二分图，其中奇数层在一侧，偶数层在另一侧。容易发现，当我们条件于偶数层中的变量时，奇数层中的变量变得条件独立。当然，当我们条件于奇数层中的变量时，偶数层中的变量也会变得条件独立。
- DBM 的二分图结构意味着我们可以应用之前用于 RBM 条件分布的相同式子来确定 DBM 中的条件分布。在给定相邻层值的情况下，层内的单元彼此条件独立，因此二值变量的分布可以由 Bernoulli 参数(描述每个单元的激活概率)完全描述。
- 在具有两个隐藏层的示例中，激活概率由下式给出：
$$P(v_i=1|h^{(1)})=\sigma(W_{i,:}^{(1)}h^{(1)})$$
$$P(h_i^{(1)}=1|v,h^{(2)})=\sigma(v^{\top}W_{:,i}^{(1)}+W_{i,:}^{(2)}h^{(2)})$$
$$P(h_k^{(2)}=1|h^{(1)})=\sigma({h^{(1)}}^{\top}W_{:,k}^{(2)})$$
- 二分图结构使 Gibbs 采样能在深度玻尔兹曼机中高效采样。给定偶数层，关于奇数层的分布是因子的，因此可以作为块同时且独立地采样。类似地，给定奇数层，可以同时且独立地将偶数层作为块进行采样。高效采样对使用随机最大似然算法的训练尤其重要。
- DBM 在其隐藏单元上的后验分布(复杂的)很容易用均匀场近似来近似。均匀场近似是变分推断的简单形式，其中我们将近似分布限制为完全因子的分布。
#### 逐层预训练
- DBM 的每一层被单独视为 RBM，进行训练。第一层被训练为对输入数据进行建模。每个后续 RBM 被训练为对来自前一 RBM 后验分布的样本进行建模。在以这种方式训练了所有 RBM 之后，它们可以被组合成 DBM。在贪心逐层训练过程中，我们在每个步骤都使用了不同的目标函数。
- DBM 的贪心逐层预训练与 DBN 的贪心逐层预训练不同。每个单独的 RBM 的参数可以直接复制到相应的 DBN。在 DBM 的情况下，RBM 的参数在包含到 DBM 中之前必须修改。RBM 栈的中间层仅使用自底向上的输入进行训练，但在栈组合形成 DBM 后，该层将同时具有自底向上和自顶向下的输入。
- 每个中间层在计算过程中被计算了两遍，出现double counting问题，因此需要做几何平均，最简单的方法是将所有 RBM (顶部和底部 RBM 除外)的权重除 2。
- 另外，必须使用每个可见单元的两个"副本"来训练底部 RBM，并且两个副本之间的权重约束为相等。这意味着在向上传播时，权重能有效地加倍。类似地，顶部 RBM 应当使用最顶层的两个副本来训练。


## 1.3 自编码器(autoencoder)
- 自编码器是神经网络的一种，内部有一个隐藏层 $h$ 可以产生编码(code)表示输入。该网络可以看成两部分组成：一个由函数 $h=f(x)$ 表示的编码器和一个生成重构的解码器 $r=g(h)$ 。
- 现代自编码器将编码器和解码器的概念推而广之，将其中的确定函数推广为随机映射 $p_{\rm{encoder}}(h|x)$ 和 $p_{\rm{decoder}}(x|h)$。
### 1.3.1 欠完备自编码器
- 从自编码器获得有用特征的一种方法是限制 $h$ 的维度比 $x$ 小，称为欠完备。学习过程可以简单描述为最小化一个损失函数：
$$L(x,g(f(x)))$$
其中 $L$ 是一个损失函数。
- 拥有非线性编码器函数 $f$ 和非线性解码器函数 $g$ 的自编码器能够学习出更强大的PCA非线性推广。但是，如果自编码器的容量太大，$L$ 仅仅使得 $g{\circ}f$ 学成一个恒等函数，那训练来执行复制任务的自编码器可能无法学习到数据集的任何有用信息。
### 1.3.2 正则自编码器
- 稀疏自编码器：在训练时结合编码层的稀疏惩罚 $\Omega(h)$ 和重构误差：
$$L(x,g(f(x)))+\Omega(h)$$
其中 $g(h)$ 是解码器的输出，通常 $h$ 是编码器的输出，即 $h=f(x)$。
    - 我们可以简单地将惩罚项 $\Omega(h)$ 视为加到前馈网络的正则项，正则自编码器的正则项取决于数据，因此根据定义上来说，它不是一个先验。
    - 另一方面，我们可以认为整个稀疏自编码器框架是对带有潜变量的生成模型的近似最大似然训练，而不将稀疏惩罚视为复制任务的正则化。在数学上可以近似推断出：稀疏惩罚对应于有向模型 $p_{\rm{model}}(x,h)=p_{\rm{model}}(h)p_{\rm{model}}(x|h)$ 中的最大似然函数 ${\rm{log}}p_{\rm{model}}(h)$。
- 去噪自编码器：通过改变重构误差项来训练：
$$L(x,g(f(\tilde{x})))$$
其中 $\tilde{x}$ 是被某种噪声损坏的 $x$ 的副本。去噪自编码器必须撤销这些损坏，而不是简单地复制输入。
- 收缩自编码器：使用类似稀疏自编码器的惩罚项，但形式不同：
$$L(x,g(f(x)))+\Omega(h,x)$$
$$\Omega(h,x)=\lambda\sum\limits_i||\nabla_xh_i||^2$$
### 1.3.3 深度自编码器
- 根据万能近似定理，深度自编码器在给定足够多的隐藏单元的情况下，能以任意精度近似任何从输入到编码的映射。深度可以指数地降低表示某些函数的计算成本，也能指数地减少学习一些函数所需的训练数据量。
### 1.3.4 随机编码器和解码器
- 任何潜变量模型 $p_{\rm{model}}(h,x)$ 定义一个随机编码器和一个随机解码器：
$$p_{\rm{encoder}}(h|x)=p_{\rm{model}}(h|x)$$
$$p_{\rm{decoder}}(x|h)=p_{\rm{model}}(x|h)$$
通常情况下，编码器和解码器的分布没有必要是与唯一一个联合分布 $p_{\rm{model}}(x,h)$ 相容的条件分布。在保证足够的容量和样本的情况下，将编码器和解码器作为去噪自编码器训练，能使它们渐进地相容。

### 1.3.5 变分自编码器(Variational autoencoder)
#### 近似推断
在深度学习中，通常我们有一系列可见变量 $v$ 和一系列潜变量 $h$，推断困难通常是指难以计算 $p(h|v)$ 或其期望。
- 精确推断问题可以描述为一个优化问题。
    - 假设我们有一个包含可见变量 $v$ 和潜变量 $h$  的概率模型，我们希望计算观察数据的对数概率 ${\rm{log}}p(v;\theta)$。有时候如果边缘化消去 $h$ 的操作很费时，我们会难以计算 ${\rm{log}}p(v;\theta)$。作为替代，我们可以计算一个 ${\rm{log}}p(v;\theta)$ 的下界 $\mathcal{L}(v,\theta,q)$。这个下界被称为证据下界(ELBO,evidence lower bound)：
    $$\mathcal{L}(v,\theta,q)={\rm{log}}p(v;\theta)-D_{KL}\left(q(h|v)\|p(h|v;\theta)\right)$$
    其中 $q$ 是关于 $h$ 的一个任意概率分布。
    - 通过简单的代数运算我们可以把 $\mathcal{L}$ 重写成更简单的形式：
    $$\begin{align*}
    \mathcal{L}(v,\theta,q)&={\rm{log}}p(v;\theta)-D_{KL}(q(h|v)\|p(h|v;\theta))\\
    &={\rm{log}}p(v;\theta)-\mathbb{E}_{h{\sim}q}{\rm{log}}\frac{q(h|v)}{p(h|v)}\\
    &={\rm{log}}p(v;\theta)-\mathbb{E}_{h{\sim}q}{\rm{log}}\frac{q(h|v)}{\frac{p(h,v;\theta)}{p(v;\theta)}}\\
    &={\rm{log}}p(v;\theta)-\mathbb{E}_{h{\sim}q}[{\rm{log}}q(h|v)-{\rm{log}}p(h,v;\theta)+{\rm\log}p(v;\theta)]\\
    &=-\mathbb{E}_{h{\sim}q}[{\rm{log}}q(h|v)-{\rm{log}}p(h,v;\theta)]
    \end{align*}$$
    这也给出了证据下界的标准定义：
    $$\mathcal{L}(v,\theta,q)=\mathbb{E}_{h{\sim}q}[{\rm{log}}p(h,v)]+H(q)$$
    - 我们可以将推断问题看作是找一个分布 $q$ 使得 $\mathcal{L}$ 最大的过程。
- EM算法：
    - E步：$\theta^{(0)}$ 是初始参数，对(所有的或小批量的)训练样本 $v^{(i)}$，令 $q(h^{(i)}|v)=p(h^{(i)}|v^{(i)};\theta^{(0)})$。
    - M步：使用选择的优化算法完全地或者部分地关于 $\theta$ 最大化 $\sum\limits_i\mathcal{L}(v^{(i)},\theta,q)$
    - 在第一步中，我们更新分布 $q$ 来最大化 $\mathcal{L}$，而在另一步中，我们更新 $\theta$ 来最大化 $\mathcal{L}$。
- 最大后验推断(MAP推断)：$h^*=\mathop{\rm{argmax}}\limits_hp(h|v)$

#### 变分的概念
- 泛函通常是指定义域为函数，值域为实数的"函数"。</br>
给定表示(连续/平滑)函数 $\rho$ (具有某些便捷条件等)的流形 $M$，则一个泛函 $F$ 可以定义为：
$$F:M\rightarrow\mathbb{R}\quad{\rm{or}}{\quad}F:M\rightarrow\mathbb{C}$$
- 泛函 $F[\rho]$ 的泛函导数(或变分导数)，记为 $\frac{{\delta}F}{{\delta}\rho}$，定义为：
$${\delta}F[\rho;\phi]=\int\frac{{\delta}F}{{\delta}\rho}(x)\phi(x)dx=\mathop{\rm{lim}}\limits_{\epsilon\rightarrow{0}}\frac{F[\rho+\epsilon\phi]-F[\rho]}{\epsilon}=\left[\frac{d}{d\epsilon}F[\rho+\epsilon\phi]\right]_{\epsilon=0}$$
其中 $\phi$ 是 $\rho$ 的变化 $\phi=\delta\rho$。

#### 变分推断
为了关于一个向量优化某个函数，我们求出了这个函数关于这个向量的梯度，然后找这个梯度中每一个元素都为 0 的点。类似地，我们可以通过寻找一个函数使得泛函导数的每个点都等于 0 从而来优化一个泛函。
- 我们考虑寻找一个定义在 $x\in\mathbb{R}$ 上的有最大微分熵的概率密度函数。一个概率分布 $p(x)$ 的熵，定义如下：
$$H[p]=-\mathbb{E}_x{\rm{log}}p(x)$$
对于连续的值，这个期望可以被看作一个积分：
$$H[p]=-{\int}p(x){\rm{log}}p(x)dx$$
- 我们不能简单地仅仅关于函数 $p(x)$ 最大化 $H[p]$，因为那样的话结果可能不是一个概率分布。为了解决这个问题，我们需要使用一个拉格朗日乘子来添加一个分布 $p(x)$ 积分值为 1 的约束。同样地，当方差增大时，熵也会无限制地增加。因此，寻找哪一个分布有最大熵这个问题是没有意义的。但是，在给定固定的方差 $\sigma^2$ 时，我们可以寻找一个最大熵的分布。最后，这个问题还是欠定的，因为在不改变熵的条件下一个分布可以被随意地改变。为了获得一个唯一的解，我们再加一个约束：分布的均值必须为 $\mu$。那么这个问题的拉格朗日泛函如下：
$$\begin{align*}
L[p]&=\lambda_1({\int}p(x)dx-1)+\lambda_2(\mathbb{E}[x]-\mu)+\lambda_3(\mathbb{E}[(x-\mu)^2]-\sigma^2)+H[p]\\
&=\int\left(\lambda_1p(x)+\lambda_2p(x)x+\lambda_3p(x)(x-\mu)^2-p(x){\rm{log}}p(x)\right)dx-\lambda_1-\mu\lambda_2-\sigma^2\lambda_3
\end{align*}$$
- 为了关于 $p$ 最小化拉格朗日乘子，我们令泛函导数等于 0：
$${\forall}x,\frac{\delta}{{\delta}p(x)}\mathcal{L}=\lambda_1+\lambda_2x+\lambda_3(x-\mu)^2-1-{\rm{log}}p(x)=0$$
这个条件告诉我们 $p(x)$ 的泛函形式。通过代数运算重组上述方程，我们可以得到：
$$p(x)={\rm{exp}}(\lambda_1+\lambda_2x+\lambda_3(x-\mu)^2-1)$$
- 我们并没有直接假设 $p(x)$ 取这种形式，而是通过最小化泛函从理论上得到了这个 $p(x)$ 的表达式。为了解决这个最小化问题，我们需要选择 $\lambda$ 的值来确保所有的约束都能够满足。我们有很大的自由去选择 $\lambda$。因为只要满足约束，拉格朗日关于 $\lambda$ 这个变量的梯度就为 0。
- 为了满足所有的约束，我们可以令 $\lambda_1=1−{\rm{log}}\sigma\sqrt{2\pi},\lambda_2=0,\lambda_3=-\frac{1}{2\sigma^2}$，从而得到：
$$p(x)=\mathcal{N}(x;\mu,\sigma^2)$$
这也是当我们不知道真实的分布时总是使用正态分布的一个原因。因为正态分布拥有最大的熵，我们通过这个假定来保证了最小可能量的结构。在不同的模型上应用相同的方法可能会得到不同泛函形式的分布。

## 1.4 生成对抗网络(Generative adversarial network)
- 生成式对抗网络(generative adversarial network, GAN)是基于可微生成器网络的一种生成式建模方法。设计 GAN 的主要动机是学习过程既不需要近似推断也不需要配分函数梯度的近似。
- 生成器网络直接产生样本 $x=g(z;\theta^{(g)})$。其对手判别器网络，试图区分从训练数据抽取的样本和从生成器抽取的样本。判别器发出由 $d(x;\theta^{(d)})$ 给出的概率值，指示 $x$ 是真实训练样本而不是从模型抽取的伪造样本的概率。
- 形式化表示生成式对抗网络中学习的最简单方式是零和游戏，其中函数 $v(\theta^{(g)},\theta^{(d)})$ 确定判别器的收益。生成器接收 $−v(\theta^{(g)},\theta^{(d)})$ 作为它自己的收益。在学习期间，每个玩家尝试最大化自己的收益，因此收敛在：
$$g^*=\mathop{\rm{argmin}}\limits_g\mathop{\rm{max}}\limits_dv(g,d)$$
$v$ 的默认选择是：
$$v(\theta^{(g)},\theta^{(d)})=\mathbb{E}_{x{\sim}p_{\rm{data}}}{\rm{log}}d(x)+\mathbb{E}_{x{\sim}p_{\rm{model}}}{\rm{log}}(1-d(x))$$
这驱使判别器试图学习将样品正确地分类为真的或伪造的。同时，生成器试图欺骗分类器以让其相信样本是真实的。在收敛时，生成器的样本与实际数据不可区分，并且判别器处处都输出 $\frac{1}{2}$。然后就可以丢弃判别器。
- 在真实实验中，GAN 博弈的最佳表现形式既不是零和也不等价于最大似然，而是带有启发式动机的不同形式化。在这种最佳性能的形式中，生成器旨在增加判别器发生错误的对数概率，而不是旨在降低判别器进行正确预测的对数概率。这种重述仅仅是观察的结果，即使在判别器确信地拒绝所有生成器样本的情况下，它也能导致生成器代价函数的导数相对于判别器的对数保持很大。

## 1.5 基于流的生成模型(Flow-based generative model)
- Flow的基本思想为：假设 $x$ 是输入数据, 其分布 $p(x)$ 未知; 通过一系列transformation $z=f(x)$, 将其分布转化为一个简单的分布 $p(z)$, 通过求 $p(z)$ 以及行列式就能求出 $p(x)$。
- 这个 $f$ 可以看做是encoder. 如果这个转化可逆, 即 $x=f^{-1}(z)$ 存在, 那么 $f^{-1}$ 就是decoder。
- 从这个目标来看, 这个仿射变换 $f(\cdot)$ 要满足两个条件:
    - 可逆. 不然没有 $f^{-1}(\cdot)$；
    - 行列式好算


## 1.6 扩散模型(Diffusion model)
- 扩散模型通过向图片中添加噪声，再反向去噪，来学习一个解码器，从而达到从噪声中生成图像的目的。
- 本质上是基于马尔科夫链构造的前向过程和逆向过程，通过构造变分下界来学习一个去噪自编码器。

