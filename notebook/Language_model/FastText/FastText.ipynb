{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0291ef73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jieba\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf34ea3",
   "metadata": {},
   "source": [
    "## 数据读取、清洗、分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8a9191f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\Savarus\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.878 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "sourceTxt = '中国外交部例行记者会.txt'\n",
    "label = ['question', 'journal', 'answer']\n",
    "sentences = []\n",
    "with open(sourceTxt, 'r', encoding='utf-8') as sourceFile:\n",
    "    for line in sourceFile:\n",
    "        line = re.sub(\"[\\s+\\.\\!\\/_,$%^*(+\\\"\\')]+|[+——()?【】《》\\[\\]\\{\\}“”！，,:：。？、~@#￥%……&*（）]+\", \"\",line)\n",
    "        seg = jieba.cut(line.strip(), cut_all=False)\n",
    "        output = ' '.join(seg)\n",
    "        templabel = output.split(\" \")[0]\n",
    "        if templabel in label:\n",
    "            sentences.append(\"__label__\"+str(templabel)+\"\\t\"+\" \".join(output.split(\" \")[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0421e3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#数据打乱\n",
    "random.shuffle(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84943a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__label__answer\\t答 我们 注意 到 有关 报道 正向 美方 表达 严重 关切 我要 重申 中方 坚决 反对 美方 与 台湾 进行 任何 形式 的 军事 联系 我们 要求 美方 切实 恪守 一个 中国 原则 和 中美 三个 联合公报 规定 停止 与 台湾 方面 进行 任何 形式 的 官方 往来 和 军事 联系 慎重 妥善处理 涉台 问题',\n",
       " '__label__answer\\t华春莹 今天 的 美国 外交 已经 沦落 为 恐吓 外交 谎言 外交 制裁 外交 美国 首席 外交官 每天 都 在 撒谎 以 攻击 抹黑 中国 为生 我 想 他们 是 弄错 了 发力点 走 在 错误 危险 的 道路 上 蓬佩奥 的 野心 很大 但人贵 有 自知之明 中国 拥有 5000 年 绵延不断 的 灿烂 文化 辉煌 文明 明年 中国共产党 将 迎来 建党 100 周年 作为 一个 百年老 党 中国共产党 拥有 9200 万名 党员 还有 近 2000 万人 在 申请加入 中国共产党 根据 皮 尤 研究 中心 哈佛大学 等 国际 知名 机构 在 全球 范围 多次 进行 的 独立 民意调查 中国 人民 对 中国共产党 和 中国政府 的 支持率 一直 高居 全球 榜首 平均 在 90 以上 今年以来 正如 大家 看到 的 在 中国共产党 坚强有力 领导 下 中国 人民 万众一心 众志成城 取得 了 抗击 疫情 的 战略性 胜利 根据 新加坡 民调 显示 经此 一役 中国 人民 对 中国共产党 和 中国政府 的 满意度 和 支持率 达到 了 历史 新高 反观 美国 我 不想 作 评论 美国 媒体 已经 作 了 很多 报道 根据 美联社 与 芝加哥大学 全国 民意 研究 中心 联合 创办 的 公共事务 研究 中心 对 美国 民众 进行 的 民调 结果显示 80 的 受访 民众 认为 美国 的 发展 方向 是 错误 的 达到 了 近年来 最高 在 很多 国家 包括 美国 一些 盟友 的 民众 也 认为 美国 现行 外交政策 实际上 对 世界 构成 了 最大 威胁 有 国际 知名 学者 表示 一个 不足 250 年 建国 史 的 国家 居然 相信 自己 可以 改变 一个 拥有 4000 多年 政治文明 的 大国 这种 想法 本身 就是 极其 荒谬 和 傲慢 的 现在 世界 各国 都 在 以 极其 忧虑 的 眼光 关注 着 美国 关注 着 美国 国内 严重 的 种族歧视 社会 撕裂 以及 疫情 形势 那些 到处 散播 仇恨 和 分裂 的 美国 政客 如果 对 自己 的 国家 和 人民 还有 一点点 良知 和 责任感 就 应该 赶紧 集中精力 拯救 更 多 美国 人民 的 生命 应该 听 一 听 弗洛伊德 布莱克 等 少数 族裔 人 的 绝望 呐喊 蓬佩奥 不要 说 其他 国家 值不值得 信任 他 应该 首先 作出 切实 努力 取得 美国 人民 对 他 的 信任',\n",
       " '__label__journal\\t彭博社',\n",
       " '__label__journal\\t',\n",
       " '__label__journal\\t深圳 卫视']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1737c9de",
   "metadata": {},
   "source": [
    "## 训练数据写入文档"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c937d206",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing data to fasttext format...\n",
      "done!\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# 写入数据-fasttext格式\n",
    "def generate_model_data(sentences): \n",
    "    train_num=int(len(sentences)*0.8)\n",
    "    train_set=sentences[0:train_num]\n",
    "    test_set=sentences[train_num:-1]\n",
    "    print(\"writing data to fasttext format...\")\n",
    "    with open('./data/train_data.txt', 'w', encoding='utf-8') as out:\n",
    "        for sentence in train_set:\n",
    "            out.write(sentence+\"\\n\")\n",
    "        print(\"done!\")\n",
    "    with open('./data/test_data.txt','w', encoding='utf-8') as f:\n",
    "        for sentence in test_set:\n",
    "            f.write(sentence+'\\n')\n",
    "        print('done!')\n",
    " \n",
    " \n",
    "generate_model_data(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b5aa0a",
   "metadata": {},
   "source": [
    "## 训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d8f5872",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<fasttext.FastText._FastText at 0x1bc920f6bb0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import fasttext \n",
    "classifier = fasttext.train_supervised('./data/train_data.txt',label='__label__', wordNgrams=2,epoch=20,lr=0.1,dim=100)\n",
    " \n",
    "#参数说明\n",
    "'''\n",
    "train_supervised(input, lr=0.1, dim=100, \n",
    "                   ws=5, epoch=5, minCount=1, \n",
    "                   minCountLabel=0, minn=0, \n",
    "                   maxn=0, neg=5, wordNgrams=1, \n",
    "                   loss=\"softmax\", bucket=2000000, \n",
    "                   thread=12, lrUpdateRate=100,\n",
    "                   t=1e-4, label=\"__label__\", \n",
    "                   verbose=2, pretrainedVectors=\"\")\n",
    "'''\n",
    " \n",
    "'''\n",
    "训练一个监督模型, 返回一个模型对象\n",
    "@param input: 训练数据文件路径\n",
    "@param lr:              学习率\n",
    "@param dim:             向量维度\n",
    "@param ws:              cbow模型时使用\n",
    "@param epoch:           次数\n",
    "@param minCount:        词频阈值, 小于该值在初始化时会过滤掉\n",
    "@param minCountLabel:   类别阈值，类别小于该值初始化时会过滤掉\n",
    "@param minn:            构造subword时最小char个数\n",
    "@param maxn:            构造subword时最大char个数\n",
    "@param neg:             负采样\n",
    "@param wordNgrams:      n-gram个数\n",
    "@param loss:            损失函数类型, softmax, ns: 负采样, hs: 分层softmax\n",
    "@param bucket:          词扩充大小, [A, B]: A语料中包含的词向量, B不在语料中的词向量\n",
    "@param thread:          线程个数, 每个线程处理输入数据的一段, 0号线程负责loss输出\n",
    "@param lrUpdateRate:    学习率更新\n",
    "@param t:               负采样阈值\n",
    "@param label:           类别前缀\n",
    "@param verbose:         ??\n",
    "@param pretrainedVectors: 预训练的词向量文件路径, 如果word出现在文件夹中初始化不再随机\n",
    "@return model object\n",
    "'''\n",
    "classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08883ca2",
   "metadata": {},
   "source": [
    "## 保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd69ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save_model('./model/fasttext.bin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29711b06",
   "metadata": {},
   "source": [
    "## 模型批量预测，以及效果评估"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "603ee28c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_precision: 0.9843830665978317\n",
      "train_recall: 0.9843830665978317\n",
      "Number of train examples: 23244\n",
      "test_precision: 0.9781411359724613\n",
      "test_recall: 0.9781411359724613\n",
      "Number of test examples: 5810\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "@return [样本个数, 准确率, 召回率]\n",
    "'''\n",
    "train_result=classifier.test('./data/train_data.txt')\n",
    "print('train_precision:', train_result[1])\n",
    "print('train_recall:', train_result[2])\n",
    "print('Number of train examples:', train_result[0])\n",
    "test_result=classifier.test('./data/test_data.txt')\n",
    "print('test_precision:', test_result[1])\n",
    "print('test_recall:', test_result[2])\n",
    "print('Number of test examples:', test_result[0])  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aab8cfd",
   "metadata": {},
   "source": [
    "## 模型单例预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ace771ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('__label__answer',), array([0.97823912]))\n",
      "answer\n"
     ]
    }
   ],
   "source": [
    "label_to_cate = {1:'question', 2:'journal', 3:'answer'}\n",
    " \n",
    "# texts = '中新网'\n",
    "# texts = '乌克兰 什么 时候 投降'\n",
    "texts = '猪猪 是 一只 漂亮 的 猪猪 。'\n",
    "labels = classifier.predict(texts)\n",
    "print(labels)\n",
    "print(labels[0][0][9:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab730c0f",
   "metadata": {},
   "source": [
    "## 模型加载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a9cbad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "path = './model/fasttext.bin'\n",
    "model = fasttext.load_model(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ff12c46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
